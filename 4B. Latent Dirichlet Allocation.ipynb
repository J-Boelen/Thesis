{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Natural language (pre)processing\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "# Wordcloud for descriptive images\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# LDA packages\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "Tourists = pd.read_csv('TwitterHashtags_LDA_NL.csv', sep= ',', low_memory = False, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tourists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a lambda function to make everything lower case.\n",
    "Tourists['text'] = Tourists['text'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "Tourists['text'] = Tourists['text'].map(lambda x: re.sub('[,\\.!?@]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hyperlinks that start with HTTP.\n",
    "Tourists['text'] = Tourists['text'].map(lambda x: re.sub(r\"http\\S+\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words with less than 3 characters (stopwords)\n",
    "Tourists['text'] = Tourists['text'].map(lambda x: re.sub(r'\\b\\w{1,3}\\b', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tourists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stemming objects\n",
    "pStemmer = PorterStemmer() # English only\n",
    "lStemmer = LancasterStemmer() # English only\n",
    "# Snowball stemmers for the top 5 languages in the twitter dataset.\n",
    "dutchStemmer = SnowballStemmer(\"dutch\", ignore_stopwords = True)\n",
    "englishStemmer = SnowballStemmer(\"english\", ignore_stopwords = True)\n",
    "spanishStemmer = SnowballStemmer(\"german\", ignore_stopwords = True)\n",
    "frenchStemmer = SnowballStemmer(\"french\", ignore_stopwords = True)\n",
    "germanStemmer = SnowballStemmer(\"spanish\", ignore_stopwords = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating stemming functions that can be applied to the text in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemDutch(inputSentence):\n",
    "    \n",
    "    # Creates a list of words in the sentence\n",
    "    words = word_tokenize(inputSentence)\n",
    "    \n",
    "    # Empty list to be filled in with the stemmed words\n",
    "    stemmedSentence=[]\n",
    "    \n",
    "    # Iterate over all the words in the sentence\n",
    "    for word in words:\n",
    "        \n",
    "        # Add the individually stemmed word to the empty list \n",
    "        stemmedSentence.append(dutchStemmer.stem(word))\n",
    "        \n",
    "        # Add a space to the empty list\n",
    "        stemmedSentence.append(\" \")\n",
    "    \n",
    "    # Return the stemmed sentence\n",
    "    return \"\".join(stemmedSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemEnglish(inputSentence):\n",
    "    \n",
    "    # Creates a list of words in the sentence\n",
    "    words = word_tokenize(inputSentence)\n",
    "    \n",
    "    # Empty list to be filled in with the stemmed words\n",
    "    stemmedSentence=[]\n",
    "    \n",
    "    # Iterate over all the words in the sentence\n",
    "    for word in words:\n",
    "        \n",
    "        # Add the individually stemmed word to the empty list \n",
    "        stemmedSentence.append(englishStemmer.stem(word))\n",
    "        \n",
    "        # Add a space to the empty list\n",
    "        stemmedSentence.append(\" \")\n",
    "    \n",
    "    # Return the stemmed sentence\n",
    "    return \"\".join(stemmedSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemSpanish(inputSentence):\n",
    "    \n",
    "    # Creates a list of words in the sentence\n",
    "    words = word_tokenize(inputSentence)\n",
    "    \n",
    "    # Empty list to be filled in with the stemmed words\n",
    "    stemmedSentence=[]\n",
    "    \n",
    "    # Iterate over all the words in the sentence\n",
    "    for word in words:\n",
    "        \n",
    "        # Add the individually stemmed word to the empty list \n",
    "        stemmedSentence.append(spanishStemmer.stem(word))\n",
    "        \n",
    "        # Add a space to the empty list\n",
    "        stemmedSentence.append(\" \")\n",
    "    \n",
    "    # Return the stemmed sentence\n",
    "    return \"\".join(stemmedSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemFrench(inputSentence):\n",
    "    \n",
    "    # Creates a list of words in the sentence\n",
    "    words = word_tokenize(inputSentence)\n",
    "    \n",
    "    # Empty list to be filled in with the stemmed words\n",
    "    stemmedSentence=[]\n",
    "    \n",
    "    # Iterate over all the words in the sentence\n",
    "    for word in words:\n",
    "        \n",
    "        # Add the individually stemmed word to the empty list \n",
    "        stemmedSentence.append(frenchStemmer.stem(word))\n",
    "        \n",
    "        # Add a space to the empty list\n",
    "        stemmedSentence.append(\" \")\n",
    "    \n",
    "    # Return the stemmed sentence\n",
    "    return \"\".join(stemmedSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemGerman(inputSentence):\n",
    "    \n",
    "    # Creates a list of words in the sentence\n",
    "    words = word_tokenize(inputSentence)\n",
    "    \n",
    "    # Empty list to be filled in with the stemmed words\n",
    "    stemmedSentence=[]\n",
    "    \n",
    "    # Iterate over all the words in the sentence\n",
    "    for word in words:\n",
    "        \n",
    "        # Add the individually stemmed word to the empty list \n",
    "        stemmedSentence.append(germanStemmer.stem(word))\n",
    "        \n",
    "        # Add a space to the empty list\n",
    "        stemmedSentence.append(\" \")\n",
    "    \n",
    "    # Return the stemmed sentence\n",
    "    return \"\".join(stemmedSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization function (english only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeSentence(inputSentence):\n",
    "    \n",
    "    # Transform to lower case\n",
    "    inputSentence = inputSentence.lower()\n",
    "    \n",
    "    # Punctuations \n",
    "    punctuations = \"?:!.,;\"\n",
    "    \n",
    "    # Tokenize the words\n",
    "    words = nltk.word_tokenize(inputSentence)\n",
    "    \n",
    "    # Remove word if it is a punctuation or excluded word\n",
    "    for word in words:\n",
    "        if word in punctuations:\n",
    "            words.remove(word)\n",
    "    \n",
    "    # Empty array of lemmatized words, to be filled in by the loop.\n",
    "    lemmatizedSentence = []\n",
    "    \n",
    "    # Iterate over all the words in the sentence and apply lemmatization\n",
    "    for word in words:\n",
    "        \n",
    "        # Add the individually stemmed word to the empty list \n",
    "        lemmatizedSentence.append(lemmatizer.lemmatize(word, pos=\"v\")) # Pos is very important!\n",
    "        \n",
    "        # Add a space to the empty list\n",
    "        lemmatizedSentence.append(\" \")\n",
    "    \n",
    "    # Return the lemmatized sentence\n",
    "    return \"\".join(lemmatizedSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions to stem the sentences and create a new column (only the first time)\n",
    "Tourists['text_processed'] = Tourists['text'].map(lambda x: stemDutch(x)) # Dutch stemming\n",
    "Tourists['text_processed'] = Tourists['text_processed'].map(lambda x: stemEnglish(x)) # English\n",
    "Tourists['text_processed'] = Tourists['text_processed'].map(lambda x: stemSpanish(x)) # Spanish\n",
    "Tourists['text_processed'] = Tourists['text_processed'].map(lambda x: stemFrench(x)) # French\n",
    "Tourists['text_processed'] = Tourists['text_processed'].map(lambda x: stemGerman(x)) # German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud to give an idea of the processed text.\n",
    "\n",
    "# Create a string with all the tweets\n",
    "allWords = ','.join(list(Tourists['text_processed'].values))\n",
    "\n",
    "# Instantiate a wordcloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Fill in the wordcloud\n",
    "wordcloud.generate(allWords)\n",
    "\n",
    "# Display the wordcloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Download package that is needed for the function\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to process the text more, maintaining only nouns or adjectives or both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNouns(inputText):\n",
    "    \n",
    "    # Lambda function to check if the word is a noun\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    \n",
    "    # Split up the sentence into words\n",
    "    words = word_tokenize(inputText)\n",
    "    \n",
    "    # Get only the nouns\n",
    "    nouns = [word for (word, pos) in pos_tag(words) if is_noun(pos)] \n",
    "    \n",
    "    # Return the nouns\n",
    "    return ' '.join(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNounsAndAdj(inputText):\n",
    "    \n",
    "    # Lambda function to check if the word is a noun or an adjective\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    \n",
    "    # Split up the sentence into words\n",
    "    words = word_tokenize(inputText)\n",
    "    \n",
    "    # Get only nouns and adjectives\n",
    "    nounsAndAdj = [word for (word, pos) in pos_tag(words) if is_noun_adj(pos)] \n",
    "    \n",
    "    # Return the nouns and adjectives\n",
    "    return ' '.join(nounsAndAdj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two new, one with nouns, one with nouns and adjectives. This way there are three dataframes to perform LDA on.\n",
    "Tourists_Nouns = Tourists.copy()\n",
    "Tourists_NounsAndAdj = Tourists.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lambda function to get only nouns\n",
    "Tourists_Nouns['nouns'] = Tourists_Nouns['text_processed'].map(lambda x: getNouns(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lambda function to get nouns and adjectives\n",
    "Tourists_NounsAndAdj['nouns'] = Tourists_NounsAndAdj['text_processed'].map(lambda x: getNounsAndAdj(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the dataframes to the final form for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Tourists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep two columns.\n",
    "Tourists = Tourists[['hashtag', 'text_processed']]\n",
    "Tourists_Nouns = Tourists_Nouns[['hashtag', 'nouns']]\n",
    "Tourists_NounsAndAdj = Tourists_NounsAndAdj[['hashtag','nouns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hashtag as index.\n",
    "Tourists.set_index('hashtag', inplace=True)\n",
    "Tourists_Nouns.set_index('hashtag', inplace=True)\n",
    "Tourists_NounsAndAdj.set_index('hashtag', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stopwords\n",
    "add_stop_words = ['stopwords']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count vectorizer object including some stopwords\n",
    "cvn = CountVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sparse matrix with the nouns data\n",
    "SparseMatrix = cvn.fit_transform(Tourists.text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document term matrix\n",
    "DocumentTermMatrix = pd.DataFrame(SparseMatrix.toarray(), columns=cvn.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index of the document term matrix to match the original index\n",
    "DocumentTermMatrix.index = Tourists.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the document term matrix\n",
    "DocumentTermMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(DocumentTermMatrix.transpose()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary dictionary\n",
    "vocabulary = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct the LDA\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=20, id2word=vocabulary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the topics of the LDA\n",
    "test = lda.show_topics(num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_transformed = lda[corpus]\n",
    "# Get the list of topic probabilities per document (per tweet)\n",
    "topicList = list(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a highest topic list\n",
    "highestTopicList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most likely topic for each tweet.\n",
    "for i in topicList:\n",
    "    \n",
    "    # Get the number of topics.\n",
    "    numberOfTopics = len(i)\n",
    "    \n",
    "    # Set the highest topic and topic probability to -1, will be changed in the loop below.\n",
    "    highestTopic = -1\n",
    "    highestTopicProbability = -1\n",
    "    \n",
    "    # Iterate over the topics\n",
    "    for j in range(0, numberOfTopics):\n",
    "        \n",
    "        # Only if the next topic scores higher than the one before, change the topic.\n",
    "        if i[j][1] > highestTopicProbability:\n",
    "            highestTopic = i[j][0] # Set the highest topic.\n",
    "            highestTopicProbability = i[j][1] # Set the highest topic probability.\n",
    "    \n",
    "    # Append the highest topic to the list\n",
    "    highestTopicList.append(highestTopic)\n",
    "    \n",
    "    # Reset the values just in case.\n",
    "    highestTopic = -1\n",
    "    highestTopicProbability = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the highest topic list and indexes together\n",
    "topicPerPost = list(zip(highestTopicList, Tourists.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an empty dictionary\n",
    "topicDictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over topicPerTweet to 1) assign topic names and 2) create a dictionary.\n",
    "for i in topicPerPost:\n",
    "    topicDictionary[i[1]] = 'Dutch Topic ' + str(i[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a topic dataframe.\n",
    "topicDataframe = pd.DataFrame(topicDictionary, index=[0])\n",
    "topicDataframe = topicDataframe.T\n",
    "topicDataframe = topicDataframe.rename(columns={0: \"topic\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the topic distribution over the hashtags.\n",
    "topicDataframe['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export, import, export to fill in the right column names.\n",
    "topicDataframe.to_csv('Topics_Twitter_NL.csv')\n",
    "topicDataframe_Imported = pd.read_csv('Topics_Twitter_NL.csv', sep= ',', low_memory = False, lineterminator='\\n')\n",
    "topicDataframe_Imported.rename(columns = {'Unnamed: 0':'hashtag'}, inplace=True)\n",
    "topicDataframe_Imported.to_csv('Topics_Twitter_NL.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning the topics to the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topicDataframe_Imported.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the correct dataset\n",
    "Tourists_EN = pd.read_csv('TwitterHashtags_LDA_NL.csv', sep= ',', low_memory = False, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "topicDataframe_Imported = pd.read_csv('Topics_Twitter_NL.csv', sep= ',', low_memory = False, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tourists_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the sets.\n",
    "Hashtag_Merged = pd.merge(Tourists_EN, topicDataframe_Imported, on='hashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv for reuse.\n",
    "Hashtag_Merged.to_csv('TwitterHashtags_LDA_EN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hashtag_Merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tourists.\n",
    "Tourists = pd.read_csv('tourists_total.csv', sep= ',', low_memory = False, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the english tourists.\n",
    "Tourists = Tourists[Tourists['lang'] == 'nl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to story the topic and tweet matches.\n",
    "TopicsPerTweet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill in this dictionary in a loop.\n",
    "for index, row in Hashtag_Merged.iterrows():\n",
    "    \n",
    "    # Create a list of the matched tweets.\n",
    "    matched_items = row['matched_items'].split(',')\n",
    "    \n",
    "    # Iterate over that list to find the tweets.\n",
    "    for i in matched_items:\n",
    "        \n",
    "        # Transform the string to an item number int to match it with the tourist dataset.\n",
    "        item_number = int(i.strip())\n",
    "        \n",
    "         # List comprehesion to get the first item of each item.\n",
    "        item_number_temp = [x[0] for x in TopicsPerTweet]\n",
    "        \n",
    "        # If the item_number is already in the list, concatenate the topic\n",
    "        if item_number in item_number_temp:\n",
    "            \n",
    "            # Get the index.\n",
    "            indexOfItem = item_number_temp.index(item_number)\n",
    "            \n",
    "            # Update the values that belong to this index/hashtag\n",
    "            TopicsPerTweet[indexOfItem][1] += (',' + str(row['topic'])) # Add the topic.\n",
    "        \n",
    "        # Else just add it to the list.\n",
    "        else:\n",
    "            TopicsPerTweet.append([item_number, str(row['topic'])])\n",
    "    \n",
    "    # Print the current progress.\n",
    "    print((index/len(Hashtag_Merged)))\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all the information.\n",
    "TopicsPerTweet_DF = pd.DataFrame(TopicsPerTweet, columns = ['item_number', 'topics'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TopicsPerTweet_Cleaned = []\n",
    "# Iterate over the newly created dataframe to combine non-unique topics.\n",
    "for index, row in TopicsPerTweet_DF.iterrows():\n",
    "    \n",
    "    # Get all the topics in a list.\n",
    "    topics = row['topics'].split(',')\n",
    "    \n",
    "    # Get the unique topics by using a list and turning that into a list.\n",
    "    topicsUnique = list(set(topics))\n",
    "    \n",
    "    topicString = ', '.join(topicsUnique)\n",
    "    \n",
    "    TopicsPerTweet_Cleaned.append([row['item_number'], topicString])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaned dataframe with all the information.\n",
    "TopicsPerTweet_Cleaned_DF = pd.DataFrame(TopicsPerTweet_Cleaned, columns = ['item_number', 'topics'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find length of a string to use in a lambda function.\n",
    "def findLen(str): \n",
    "    counter = 0    \n",
    "    for i in str: \n",
    "        counter += 1\n",
    "    return counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsPerTweet_Cleaned_DF['length'] = TopicsPerTweet_Cleaned_DF['topics'].map(lambda x: findLen(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsPerTweet_Cleaned_OnlySingleTopic = TopicsPerTweet_Cleaned_DF[TopicsPerTweet_Cleaned_DF['length'] < 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsPerTweet_Cleaned_OnlySingleTopic['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsPerTweet_Cleaned_MultipleTopics = TopicsPerTweet_Cleaned_DF[TopicsPerTweet_Cleaned_DF['length'] >= 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probabilityDistributionTopics = [0.36348639, 0.079136151, 0.079961007, 0.055565276, 0.0537156, 0.047416702, 0.038393281, 0.031669458, 0.031594471, 0.028869948, 0.026720324, 0.024995626, 0.022096133, 0.02177119, 0.021221286, 0.020546404, 0.017396956, 0.016697078, 0.011672957, 0.007073762]\n",
    "MultipleTopicsPerTweet_Cleaned = []\n",
    "\n",
    "# Assign just one topic to those tweets based on the probability distribution of the hashtags.\n",
    "for index, row in TopicsPerTweet_Cleaned_MultipleTopics.iterrows():\n",
    "    \n",
    "    # Get the topics\n",
    "    topics = row['topics'].split(', ')\n",
    "    \n",
    "    # Variables to fill in based on probability distribution.\n",
    "    highestProbability = -1\n",
    "    topic = ''\n",
    "    \n",
    "    # Iterate over the topics.\n",
    "    for i in topics:\n",
    "        \n",
    "        # Get the topic number.\n",
    "        topic = int(i.split(' ')[2])\n",
    "        \n",
    "        # Find the probability in the distribution.\n",
    "        probability = probabilityDistributionTopics[topic - 1]\n",
    "        \n",
    "        # If the probablity is higher than the one before, change the variables.\n",
    "        if probability > highestProbability:\n",
    "            highestProbability = probability\n",
    "            topic = i\n",
    "    \n",
    "    MultipleTopicsPerTweet_Cleaned.append([row['item_number'], i])\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaned dataframe with all the information.\n",
    "MultipleTopicsPerTweet_Cleaned_DF = pd.DataFrame(MultipleTopicsPerTweet_Cleaned, columns = ['item_number', 'topics'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine everything into one final dataframe.\n",
    "frames = [MultipleTopicsPerTweet_Cleaned_DF, TopicsPerTweet_Cleaned_OnlySingleTopic]\n",
    "TopicsPerTweet_Final_EN = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsPerTweet_Final_EN.drop(columns='length', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsPerTweet_Final_EN.rename(columns={'topics':'topic'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv for reuse.\n",
    "TopicsPerTweet_Final_EN.to_csv('tweets_topic_NL.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
